{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9A4HuVIYGgXg"
      },
      "source": [
        "# Processo Seletivo Engenheiro de LLM\n",
        "versão 19 de janeiro de 2025"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ktJJqQ4SjvB0"
      },
      "source": [
        "### Nome: Nilton Seixas\n",
        "\n",
        "### E-mail:nfsseixas@gmail.com"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XRnOFwqk23W4"
      },
      "source": [
        "## Instalação e importação de pacotes"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HA5BWLDCKmw3",
        "outputId": "b4c8e558-a4fb-4ba3-beae-f8fa2057fc9e"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\u001b[?25l   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/480.6 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K   \u001b[91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[90m╺\u001b[0m \u001b[32m471.0/480.6 kB\u001b[0m \u001b[31m21.0 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\r\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m480.6/480.6 kB\u001b[0m \u001b[31m10.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h\u001b[?25l   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/116.3 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m116.3/116.3 kB\u001b[0m \u001b[31m8.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m179.3/179.3 kB\u001b[0m \u001b[31m13.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m143.5/143.5 kB\u001b[0m \u001b[31m8.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m194.8/194.8 kB\u001b[0m \u001b[31m14.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "torch 2.5.1+cu124 requires nvidia-cublas-cu12==12.4.5.8; platform_system == \"Linux\" and platform_machine == \"x86_64\", but you have nvidia-cublas-cu12 12.5.3.2 which is incompatible.\n",
            "torch 2.5.1+cu124 requires nvidia-cuda-cupti-cu12==12.4.127; platform_system == \"Linux\" and platform_machine == \"x86_64\", but you have nvidia-cuda-cupti-cu12 12.5.82 which is incompatible.\n",
            "torch 2.5.1+cu124 requires nvidia-cuda-nvrtc-cu12==12.4.127; platform_system == \"Linux\" and platform_machine == \"x86_64\", but you have nvidia-cuda-nvrtc-cu12 12.5.82 which is incompatible.\n",
            "torch 2.5.1+cu124 requires nvidia-cuda-runtime-cu12==12.4.127; platform_system == \"Linux\" and platform_machine == \"x86_64\", but you have nvidia-cuda-runtime-cu12 12.5.82 which is incompatible.\n",
            "torch 2.5.1+cu124 requires nvidia-cudnn-cu12==9.1.0.70; platform_system == \"Linux\" and platform_machine == \"x86_64\", but you have nvidia-cudnn-cu12 9.3.0.75 which is incompatible.\n",
            "torch 2.5.1+cu124 requires nvidia-cufft-cu12==11.2.1.3; platform_system == \"Linux\" and platform_machine == \"x86_64\", but you have nvidia-cufft-cu12 11.2.3.61 which is incompatible.\n",
            "torch 2.5.1+cu124 requires nvidia-curand-cu12==10.3.5.147; platform_system == \"Linux\" and platform_machine == \"x86_64\", but you have nvidia-curand-cu12 10.3.6.82 which is incompatible.\n",
            "torch 2.5.1+cu124 requires nvidia-cusolver-cu12==11.6.1.9; platform_system == \"Linux\" and platform_machine == \"x86_64\", but you have nvidia-cusolver-cu12 11.6.3.83 which is incompatible.\n",
            "torch 2.5.1+cu124 requires nvidia-cusparse-cu12==12.3.1.170; platform_system == \"Linux\" and platform_machine == \"x86_64\", but you have nvidia-cusparse-cu12 12.5.1.3 which is incompatible.\n",
            "torch 2.5.1+cu124 requires nvidia-nvjitlink-cu12==12.4.127; platform_system == \"Linux\" and platform_machine == \"x86_64\", but you have nvidia-nvjitlink-cu12 12.5.82 which is incompatible.\n",
            "gcsfs 2024.10.0 requires fsspec==2024.10.0, but you have fsspec 2024.9.0 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[0m"
          ]
        }
      ],
      "source": [
        "!pip install datasets -q"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 45,
      "metadata": {
        "id": "VorDvF62iyXF"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import random\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "\n",
        "from collections import Counter\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from datasets import load_dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 46,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_D9YhUAEe8eg",
        "outputId": "ea0df0ce-f060-4eba-f739-da60f68ad249"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package punkt to /home/nilton/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "execution_count": 46,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "import nltk\n",
        "nltk.download(\"punkt\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 47,
      "metadata": {
        "id": "edkOoLMqfaso"
      },
      "outputs": [],
      "source": [
        "from nltk.tokenize import word_tokenize"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 48,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "m9l6yjiYgEbX",
        "outputId": "4bae1499-211e-42bb-956c-cf1d67f1c4ec"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package punkt_tab to /home/nilton/nltk_data...\n",
            "[nltk_data]   Package punkt_tab is already up-to-date!\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "execution_count": 48,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "nltk.download('punkt_tab')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "A5ovJE02CwKT"
      },
      "source": [
        "## I - Vocabulário e Tokenização"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "A8IQxzbyZyA1"
      },
      "source": [
        "### Exemplo do dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 49,
      "metadata": {
        "id": "MtwgK6vnZnQ8"
      },
      "outputs": [],
      "source": [
        "train_dataset = load_dataset(\"stanfordnlp/imdb\", split=\"train\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 50,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fT1fRvkEIavn",
        "outputId": "ac400574-bfc2-45de-dacd-0d68e6dd9338"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "25000"
            ]
          },
          "execution_count": 50,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "len(train_dataset)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 51,
      "metadata": {
        "id": "mqzUqy3diz0X"
      },
      "outputs": [],
      "source": [
        "# limit the vocabulary size to 20000 most frequent tokens\n",
        "vocab_size = 20000\n",
        "## Corrigindo o tokenizador\n",
        "counter = Counter()\n",
        "for sample in train_dataset:\n",
        "    counter.update(word_tokenize(sample[\"text\"].lower())) ## usando o nltk para tokenizar\n",
        "\n",
        "# create a vocabulary of the 20000 most frequent tokens\n",
        "most_frequent_words = sorted(counter, key=counter.get, reverse=True)[:vocab_size]\n",
        "vocab = {word: i for i, word in enumerate(most_frequent_words, 1)}\n",
        "vocab_size = len(vocab)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 52,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0rZn-m1Mi110",
        "outputId": "b4fa4002-47ce-44da-ed1e-6748afd785fe"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "[15, 49, 7985, 3]"
            ]
          },
          "execution_count": 52,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "def encode_sentence(sentence, vocab):\n",
        "    words = word_tokenize(sentence.lower())  # Agora considera pontuação e palavras minúsculas\n",
        "    return [vocab.get(word, 0) for word in words]\n",
        "\n",
        "encode_sentence(\"I like Pizza.\", vocab)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Me7-TCPVezEz"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5iV4bF8cDAj1"
      },
      "source": [
        "## II - Dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 87,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VDUyZoTPi262",
        "outputId": "b3a0a99c-2703-4f59-c8b9-02ed4d1b480d"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "(25000, 25000)"
            ]
          },
          "execution_count": 87,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "from torch.nn.functional import one_hot\n",
        "# Dataset Class with One-hot Encoding\n",
        "class IMDBDataset(Dataset):\n",
        "    def __init__(self, split, vocab):\n",
        "        self.data = load_dataset(\"stanfordnlp/imdb\", split=split)\n",
        "        self.vocab = vocab\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.data)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        sample = self.data[idx]\n",
        "        target = torch.tensor(sample[\"label\"], dtype=torch.float32)\n",
        "\n",
        "        indices = []\n",
        "        for word in encode_sentence(sample[\"text\"], self.vocab):\n",
        "            indices.append(word)\n",
        "            \n",
        "        values = torch.ones(len(indices))  \n",
        "        indices = torch.tensor(indices).unsqueeze(0)  \n",
        "        X = torch.sparse.FloatTensor(indices, values, torch.Size([len(self.vocab) + 1]))\n",
        "\n",
        "        return X, target\n",
        "\n",
        "# Load Data with One-hot Encoding\n",
        "batch_size = 128\n",
        "train_data = IMDBDataset('train', vocab)\n",
        "test_data = IMDBDataset('test', vocab)\n",
        "\n",
        "len(train_data), len(test_data)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 88,
      "metadata": {
        "id": "JwcM1zJKtG-H"
      },
      "outputs": [],
      "source": [
        "def sparse_collate(batch):\n",
        "    inputs, targets = zip(*batch)  # Separate inputs and targets\n",
        "\n",
        "    # Convert sparse tensors to dense\n",
        "    dense_inputs = [i.to_dense() for i in inputs]\n",
        "\n",
        "    # Stack dense tensors into a batch\n",
        "    batch_inputs = torch.stack(dense_inputs)\n",
        "    batch_targets = torch.tensor(targets, dtype=torch.float32)\n",
        "\n",
        "    return batch_inputs, batch_targets"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "d7RMPSvMDL5U"
      },
      "source": [
        "## III - Data Loader"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 89,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Y7tcZv2YDIog",
        "outputId": "c7d6a24f-2275-44bd-e7c2-60fc0759565f"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/home/nilton/RAG01/.venv/lib/python3.9/site-packages/torch/utils/data/dataloader.py:617: UserWarning: This DataLoader will create 16 worker processes in total. Our suggested max number of worker in current system is 8, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
            "  warnings.warn(\n"
          ]
        }
      ],
      "source": [
        "train_loader = DataLoader(train_data, batch_size=batch_size, shuffle=True, num_workers=16, pin_memory=True, persistent_workers=True, collate_fn=sparse_collate)\n",
        "test_loader  = DataLoader(test_data, batch_size=batch_size, shuffle=False, num_workers=16, pin_memory=True, persistent_workers=True, collate_fn=sparse_collate)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MwPeJ7h8DahT"
      },
      "source": [
        "## IV - Modelo"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 90,
      "metadata": {
        "id": "6QuDhWvji7lt"
      },
      "outputs": [],
      "source": [
        "class OneHotMLP(nn.Module):\n",
        "    def __init__(self, vocab_size):\n",
        "        super(OneHotMLP, self).__init__()\n",
        "\n",
        "        self.fc1 = nn.Linear(vocab_size+1, 200)\n",
        "        self.fc2 = nn.Linear(200, 1)\n",
        "\n",
        "        self.relu = nn.ReLU()\n",
        "\n",
        "    def forward(self, x):\n",
        "        o = self.fc1(x.float())\n",
        "        o = self.relu(o)\n",
        "        return self.fc2(o)\n",
        "\n",
        "# Model instantiation\n",
        "#model = OneHotMLP(vocab_size)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 97,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WFq5D02_l8NS",
        "outputId": "6962277a-b410-4bf4-d955-c85339772db3"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "cuda\n"
          ]
        }
      ],
      "source": [
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "model = OneHotMLP(vocab_size).to(device)\n",
        "print(device)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iVAhdFGXDepU"
      },
      "source": [
        "## V - Laço de Treinamento - Otimização da função de Perda pelo Gradiente descendente"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 92,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RaH1Uv3yHih5",
        "outputId": "c759bb0f-2055-4ab3-d332-6e65d72ea3d5"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "GPU: NVIDIA L40S\n"
          ]
        }
      ],
      "source": [
        "# Verifica se há uma GPU disponível e define o dispositivo para GPU se possível,\n",
        "# caso contrário, usa a CPU\n",
        "\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "if device.type == 'cuda':\n",
        "    print('GPU:', torch.cuda.get_device_name(torch.cuda.current_device()))\n",
        "else:\n",
        "    print('using CPU')\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 93,
      "metadata": {
        "id": "yH3NTcpUvfMd"
      },
      "outputs": [],
      "source": [
        "torch.backends.cudnn.benchmark = True"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 94,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xdWcpPUPpjW8",
        "outputId": "6147ee7b-03b2-457c-94e9-6981618b3f62"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/tmp/ipykernel_43317/2479615527.py:2: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.\n",
            "  scaler = GradScaler()\n"
          ]
        }
      ],
      "source": [
        "from torch.cuda.amp import autocast, GradScaler\n",
        "scaler = GradScaler()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 95,
      "metadata": {},
      "outputs": [],
      "source": [
        "gradient_accumulation_steps = 4"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 98,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 428
        },
        "id": "Nh_pe8rni93_",
        "outputId": "f2e62650-9c0a-4a6c-86c8-c55095737a6c"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/tmp/ipykernel_43317/646730680.py:9: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.\n",
            "  scaler = GradScaler()\n",
            "/tmp/ipykernel_43317/646730680.py:20: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
            "  with autocast(dtype=torch.float16):\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch [1/5], Loss: 0.6312, Time: 5.43s\n",
            "Epoch [2/5], Loss: 0.5398, Time: 5.43s\n",
            "Epoch [3/5], Loss: 0.4930, Time: 5.39s\n",
            "Epoch [4/5], Loss: 0.4872, Time: 5.40s\n",
            "Epoch [5/5], Loss: 0.4486, Time: 5.40s\n"
          ]
        }
      ],
      "source": [
        "import time\n",
        "\n",
        "# Define loss and optimizer\n",
        "criterion = nn.BCEWithLogitsLoss()\n",
        "optimizer = optim.SGD(model.parameters(), lr=0.01, momentum=0.9, nesterov=True)\n",
        "\n",
        "# Training loop\n",
        "num_epochs = 5\n",
        "scaler = GradScaler()\n",
        "\n",
        "for epoch in range(num_epochs):\n",
        "    start_time = time.time()\n",
        "    model.train()\n",
        "    total_loss = 0\n",
        "    optimizer.zero_grad()\n",
        "\n",
        "    for step, (inputs, targets) in enumerate(train_loader):\n",
        "        inputs, targets = inputs.to(device, non_blocking=True), targets.to(device, non_blocking=True)\n",
        "\n",
        "        with autocast(dtype=torch.float16):\n",
        "            logits = model(inputs)\n",
        "            loss = criterion(logits.squeeze(), targets) / gradient_accumulation_steps\n",
        "\n",
        "        scaler.scale(loss).backward()\n",
        "\n",
        "        if (step + 1) % gradient_accumulation_steps == 0:\n",
        "            scaler.step(optimizer)\n",
        "            scaler.update()\n",
        "            optimizer.zero_grad()\n",
        "\n",
        "        total_loss += loss.item() * gradient_accumulation_steps\n",
        "\n",
        "    epoch_time = time.time() - start_time\n",
        "    avg_loss = total_loss / len(train_loader)\n",
        "\n",
        "    print(f\"Epoch [{epoch+1}/{num_epochs}], Loss: {avg_loss:.4f}, Time: {epoch_time:.2f}s\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "x5iynF6Eug0N"
      },
      "outputs": [],
      "source": [
        "watch -n 1 nvidia-smi"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Bwvahen5D1oM"
      },
      "source": [
        "## VI - Avaliação"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 99,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2DtTPUBfjBj-",
        "outputId": "76130136-286f-4ed3-e64d-c69fe2df4091"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/tmp/ipykernel_43317/713298243.py:21: UserWarning: torch.sparse.SparseTensor(indices, values, shape, *, device=) is deprecated.  Please use torch.sparse_coo_tensor(indices, values, shape, dtype=, device=). (Triggered internally at ../torch/csrc/utils/tensor_new.cpp:651.)\n",
            "  X = torch.sparse.FloatTensor(indices, values, torch.Size([len(self.vocab) + 1]))\n",
            "/tmp/ipykernel_43317/713298243.py:21: UserWarning: torch.sparse.SparseTensor(indices, values, shape, *, device=) is deprecated.  Please use torch.sparse_coo_tensor(indices, values, shape, dtype=, device=). (Triggered internally at ../torch/csrc/utils/tensor_new.cpp:651.)\n",
            "  X = torch.sparse.FloatTensor(indices, values, torch.Size([len(self.vocab) + 1]))\n",
            "/tmp/ipykernel_43317/713298243.py:21: UserWarning: torch.sparse.SparseTensor(indices, values, shape, *, device=) is deprecated.  Please use torch.sparse_coo_tensor(indices, values, shape, dtype=, device=). (Triggered internally at ../torch/csrc/utils/tensor_new.cpp:651.)\n",
            "  X = torch.sparse.FloatTensor(indices, values, torch.Size([len(self.vocab) + 1]))\n",
            "/tmp/ipykernel_43317/713298243.py:21: UserWarning: torch.sparse.SparseTensor(indices, values, shape, *, device=) is deprecated.  Please use torch.sparse_coo_tensor(indices, values, shape, dtype=, device=). (Triggered internally at ../torch/csrc/utils/tensor_new.cpp:651.)\n",
            "  X = torch.sparse.FloatTensor(indices, values, torch.Size([len(self.vocab) + 1]))\n",
            "/tmp/ipykernel_43317/713298243.py:21: UserWarning: torch.sparse.SparseTensor(indices, values, shape, *, device=) is deprecated.  Please use torch.sparse_coo_tensor(indices, values, shape, dtype=, device=). (Triggered internally at ../torch/csrc/utils/tensor_new.cpp:651.)\n",
            "  X = torch.sparse.FloatTensor(indices, values, torch.Size([len(self.vocab) + 1]))\n",
            "/tmp/ipykernel_43317/713298243.py:21: UserWarning: torch.sparse.SparseTensor(indices, values, shape, *, device=) is deprecated.  Please use torch.sparse_coo_tensor(indices, values, shape, dtype=, device=). (Triggered internally at ../torch/csrc/utils/tensor_new.cpp:651.)\n",
            "  X = torch.sparse.FloatTensor(indices, values, torch.Size([len(self.vocab) + 1]))\n",
            "/tmp/ipykernel_43317/713298243.py:21: UserWarning: torch.sparse.SparseTensor(indices, values, shape, *, device=) is deprecated.  Please use torch.sparse_coo_tensor(indices, values, shape, dtype=, device=). (Triggered internally at ../torch/csrc/utils/tensor_new.cpp:651.)\n",
            "  X = torch.sparse.FloatTensor(indices, values, torch.Size([len(self.vocab) + 1]))\n",
            "/tmp/ipykernel_43317/713298243.py:21: UserWarning: torch.sparse.SparseTensor(indices, values, shape, *, device=) is deprecated.  Please use torch.sparse_coo_tensor(indices, values, shape, dtype=, device=). (Triggered internally at ../torch/csrc/utils/tensor_new.cpp:651.)\n",
            "  X = torch.sparse.FloatTensor(indices, values, torch.Size([len(self.vocab) + 1]))\n",
            "/tmp/ipykernel_43317/713298243.py:21: UserWarning: torch.sparse.SparseTensor(indices, values, shape, *, device=) is deprecated.  Please use torch.sparse_coo_tensor(indices, values, shape, dtype=, device=). (Triggered internally at ../torch/csrc/utils/tensor_new.cpp:651.)\n",
            "  X = torch.sparse.FloatTensor(indices, values, torch.Size([len(self.vocab) + 1]))\n",
            "/tmp/ipykernel_43317/713298243.py:21: UserWarning: torch.sparse.SparseTensor(indices, values, shape, *, device=) is deprecated.  Please use torch.sparse_coo_tensor(indices, values, shape, dtype=, device=). (Triggered internally at ../torch/csrc/utils/tensor_new.cpp:651.)\n",
            "  X = torch.sparse.FloatTensor(indices, values, torch.Size([len(self.vocab) + 1]))\n",
            "/tmp/ipykernel_43317/713298243.py:21: UserWarning: torch.sparse.SparseTensor(indices, values, shape, *, device=) is deprecated.  Please use torch.sparse_coo_tensor(indices, values, shape, dtype=, device=). (Triggered internally at ../torch/csrc/utils/tensor_new.cpp:651.)\n",
            "  X = torch.sparse.FloatTensor(indices, values, torch.Size([len(self.vocab) + 1]))\n",
            "/tmp/ipykernel_43317/713298243.py:21: UserWarning: torch.sparse.SparseTensor(indices, values, shape, *, device=) is deprecated.  Please use torch.sparse_coo_tensor(indices, values, shape, dtype=, device=). (Triggered internally at ../torch/csrc/utils/tensor_new.cpp:651.)\n",
            "  X = torch.sparse.FloatTensor(indices, values, torch.Size([len(self.vocab) + 1]))\n",
            "/tmp/ipykernel_43317/713298243.py:21: UserWarning: torch.sparse.SparseTensor(indices, values, shape, *, device=) is deprecated.  Please use torch.sparse_coo_tensor(indices, values, shape, dtype=, device=). (Triggered internally at ../torch/csrc/utils/tensor_new.cpp:651.)\n",
            "  X = torch.sparse.FloatTensor(indices, values, torch.Size([len(self.vocab) + 1]))\n",
            "/tmp/ipykernel_43317/713298243.py:21: UserWarning: torch.sparse.SparseTensor(indices, values, shape, *, device=) is deprecated.  Please use torch.sparse_coo_tensor(indices, values, shape, dtype=, device=). (Triggered internally at ../torch/csrc/utils/tensor_new.cpp:651.)\n",
            "  X = torch.sparse.FloatTensor(indices, values, torch.Size([len(self.vocab) + 1]))\n",
            "/tmp/ipykernel_43317/713298243.py:21: UserWarning: torch.sparse.SparseTensor(indices, values, shape, *, device=) is deprecated.  Please use torch.sparse_coo_tensor(indices, values, shape, dtype=, device=). (Triggered internally at ../torch/csrc/utils/tensor_new.cpp:651.)\n",
            "  X = torch.sparse.FloatTensor(indices, values, torch.Size([len(self.vocab) + 1]))\n",
            "/tmp/ipykernel_43317/713298243.py:21: UserWarning: torch.sparse.SparseTensor(indices, values, shape, *, device=) is deprecated.  Please use torch.sparse_coo_tensor(indices, values, shape, dtype=, device=). (Triggered internally at ../torch/csrc/utils/tensor_new.cpp:651.)\n",
            "  X = torch.sparse.FloatTensor(indices, values, torch.Size([len(self.vocab) + 1]))\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Test Accuracy: 82.892%\n"
          ]
        }
      ],
      "source": [
        "## evaluation\n",
        "model.eval()\n",
        "\n",
        "with torch.no_grad():\n",
        "    correct = 0\n",
        "    total = 0\n",
        "    for inputs, targets in test_loader:\n",
        "        inputs = inputs.to(device)\n",
        "        targets = targets.to(device)\n",
        "        logits = model(inputs)\n",
        "        predicted = torch.round(torch.sigmoid(logits.squeeze()))\n",
        "        total += targets.size(0)\n",
        "        correct += (predicted == targets).sum().item()\n",
        "\n",
        "    print(f'Test Accuracy: {100 * correct / total}%')"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": [],
      "toc_visible": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
