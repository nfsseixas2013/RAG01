{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/nilton/RAG01/.venv/lib/python3.9/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "from tqdm.auto import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import BitsAndBytesConfig"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "quantizations_config = BitsAndBytesConfig(load_in_4bit=True,\n",
    "                                          bnb_4bit_compute_dtype=torch.float16)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers.utils import is_flash_attn_2_available"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "## teste se a gpu aguenta um score de > 8\n",
    "if is_flash_attn_2_available() and torch.cuda.get_device_capability(0)[0] >= 8:\n",
    "    attn_implementation = \"flash_attention_2\"\n",
    "else:\n",
    "    attn_implementation = \"sdpa\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'sdpa'"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "attn_implementation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "False"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "is_flash_attn_2_available()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_checkpoint = \"deepseek-ai/DeepSeek-R1-Distill-Llama-8B\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "8"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.cuda.get_device_capability(0)[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained(pretrained_model_name_or_path=model_checkpoint)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "use_quantization_config = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`low_cpu_mem_usage` was None, now default to True since model is quantized.\n",
      "Loading checkpoint shards: 100%|██████████| 2/2 [00:07<00:00,  3.64s/it]\n"
     ]
    }
   ],
   "source": [
    "llm_model = AutoModelForCausalLM.from_pretrained(pretrained_model_name_or_path=model_checkpoint,\n",
    "                                              #   torch_dtype= torch.float16,\n",
    "                                                 quantization_config=quantizations_config,\n",
    "                                              #   low_cpu_mem_usage = False,\n",
    "                                                 attn_implementation=attn_implementation,\n",
    "                                                 )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "device(type='cuda')"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LlamaForCausalLM(\n",
       "  (model): LlamaModel(\n",
       "    (embed_tokens): Embedding(128256, 4096)\n",
       "    (layers): ModuleList(\n",
       "      (0-31): 32 x LlamaDecoderLayer(\n",
       "        (self_attn): LlamaAttention(\n",
       "          (q_proj): Linear4bit(in_features=4096, out_features=4096, bias=False)\n",
       "          (k_proj): Linear4bit(in_features=4096, out_features=1024, bias=False)\n",
       "          (v_proj): Linear4bit(in_features=4096, out_features=1024, bias=False)\n",
       "          (o_proj): Linear4bit(in_features=4096, out_features=4096, bias=False)\n",
       "        )\n",
       "        (mlp): LlamaMLP(\n",
       "          (gate_proj): Linear4bit(in_features=4096, out_features=14336, bias=False)\n",
       "          (up_proj): Linear4bit(in_features=4096, out_features=14336, bias=False)\n",
       "          (down_proj): Linear4bit(in_features=14336, out_features=4096, bias=False)\n",
       "          (act_fn): SiLU()\n",
       "        )\n",
       "        (input_layernorm): LlamaRMSNorm((4096,), eps=1e-05)\n",
       "        (post_attention_layernorm): LlamaRMSNorm((4096,), eps=1e-05)\n",
       "      )\n",
       "    )\n",
       "    (norm): LlamaRMSNorm((4096,), eps=1e-05)\n",
       "    (rotary_emb): LlamaRotaryEmbedding()\n",
       "  )\n",
       "  (lm_head): Linear(in_features=4096, out_features=128256, bias=False)\n",
       ")"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "llm_model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_num_parametros(model: torch.nn.Module):\n",
    "    return sum([param.numel() for param in model.parameters()])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4540600320"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "get_num_parametros(model=llm_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Carregando o retrieval"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from tqdm.auto import tqdm\n",
    "from sentence_transformers import util, SentenceTransformer\n",
    "import torch\n",
    "import textwrap"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = pd.read_pickle(\"Dataset/Embedded_chunks.pkl\")\n",
    "    \n",
    "## carregando o modelo\n",
    "model = SentenceTransformer(model_name_or_path=\"all-mpnet-base-v2\", device=\"cuda\")\n",
    "\n",
    "## convertendo nossa lista de embeddings para tensor\n",
    "embeddings = torch.stack(list(dataset[\"embedding\"]))\n",
    "\n",
    "## Convertendo o dataset para lista de dicionários\n",
    "dataset = dataset.to_dict(orient=\"records\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## RAG is in the order!!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "import retrieval"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "def formatador_prompt(query: str,\n",
    "                      context_items: list[dict])-> str:\n",
    "    context = \"- \"+ \"\\n- \".join(item[\"paragrafo_chunk\"] for item in context_items)\n",
    "\n",
    "    prompt = f\"\"\"\n",
    "\n",
    "    Based on the following context items, please answer the query:\n",
    "    Context items:\n",
    "    {context}\n",
    "\n",
    "    Query: {query}\n",
    "\n",
    "    Answer:\n",
    "\n",
    "    \"\"\"\n",
    "    return prompt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prompting(query: str):\n",
    "    _, indices = retrieval.busca_contexto_relevante(query=query,\n",
    "                                               embeddings=embeddings,\n",
    "                                               model=model,\n",
    "                                               numero_de_contextos=10)\n",
    "    context_items = [dataset[i] for i in indices]\n",
    "    return formatador_prompt(query=query,context_items=context_items)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt = prompting(query=\"Describe the V's of big data\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "inputs_ids = tokenizer(prompt, return_tensors=\"pt\").to(\"cuda\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n"
     ]
    }
   ],
   "source": [
    "outputs = llm_model.generate(**inputs_ids,\n",
    "                             temperature = 0.5,\n",
    "                             do_sample= True,\n",
    "                             max_new_tokens = 512)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "output_text = tokenizer.decode(outputs[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<｜begin▁of▁sentence｜>\n",
      "\n",
      "    Based on the following context items, please answer the query:\n",
      "    Context items:\n",
      "    - In simple words, big data can be combined as three Vs.The three Vs are volume, variety and velocity.These three can be explained as follows.Volume refers to the large amount of data which is being generated at a constant rate in different industries.So, it simply is large amounts of data.The second ‘V’, variety, means that the data can be of Big Data—A New Technology Trend and Factors … 261\n",
      "- different types such as documents, images, databases.The third ‘V’ is velocity.As stated previously, the amount of data being generated is very high.For example, the number of images uploaded on Facebook last year is more than the total images in previous years [23].Therefore, it is important to consider this factor thoroughly while working on big data implementation.Apart from that, it is also important to see the speed of retrieval of data as well from these large data sets.However, big data should not be confused with ‘lots of data’.They are different and have different meanings.Big data is different from small data in many ways.The goals of big data are different from those of small data.\n",
      "- Therefore, big data helps to generate knowledge from un-used data.Big data is now a new technology trend and thus it needs very advanced hardware and software devices.Big data is very large in terms of variety, volume and velocity.Big data has ‘variety’ because the data comprises images, text, documents and every other category.These elements create a very broad range of variety and thus it contains vital information.Big data, as its name suggests, has a very large volume.The third and interesting part is its velocity.Storage plays a very important role in the case of big data.Big data cannot be stored in traditional storage systems and that is why it is important to have dedicated storage systems for this type of data.Another important factor of storage is that it should allow the prompt retrieval of data for analysis as needed.\n",
      "- All of these factors have helped to produce the big data revolution.The next section discusses the sources of big data.Authors previously explained how data are different how they are being gen- erated.In this section, the authors explain the actual sources of big data.There are different types of sources which play an important role in the generation of big data.There are mainly three parts of data which contribute in generating data.These types can be deﬁned as directed data, automated data and directed data.Directed data is a type of data which is generated in ofﬁces by employees who are usually directed by managers.This is also the type of data which is generated in labora- tories.In addition, this type of data is stored automatically and in a suitable format.\n",
      "- The goals of small data are to just answer speciﬁc questions; whereas in the case of big data, the question paradigm changes and is ﬂexible.Similarly, the location of big data and small data are also different.Small data is covered only for one institution while big data are covered between multiple servers and can be located anywhere in the world.Another obvious thing is that data contents and structure is also different.In the case of small data, the user prepares his/her own data whereas in big data, the data can come from multiple resources.Hence, big data is different from small data in its preparation.The longevity of small data and big data are also different.Small data are retained until the end of a particular project; on the other hand, big data are stored until the same amount of information is not available.Measurements of small data are done using one protocol only; however, big data can be measured using different protocols.Reproducibility is an important feature of data and the reproduction of small data is always repeatable while in the case of big data, it is seldom feasible.\n",
      "- It can be categorized in eight different types.The ﬁrst and foremost thing is big data requires a different culture than that of legacy culture.It is important that an organization have a center of all information and it is available easily for the people in IT to store and analyze.Other important thing for an organization is that it has skilled people and they have the ability to work on very large sets of data.Moreover, an organization needs to understand that data is every-where and it cannot be unseen.Organizations should follow the rule that everything which is in digital format is data.This data is the source of information.In addition to that, big data engineers are hard to ﬁnd and this is the reason that an organization should ﬁnd appropriate talent before a competitor does.Moreover, big data means much information and much information means much knowledge.In the digital age, it is important that knowledge be kept safe.\n",
      "- http://www.dataintensity.com/characteristics-of-big-data-part-one/. Accessed 31 Dec 2016.Accessed 31 Dec 2016\n",
      "- data.Thus, big data, which is still new in the technology industry, needs to be introduced on a large scale in Australia.3 Big Data Critical Factors Similarly to all other technologies, big data also has certain characteristics which differentiate big data from other technologies.These characteristics are volume, velocity, variety and complexity.Apart from these characteristics, there are other factors which play a decisive role in whether or not a company can afford big data.a. Skills Big data technology is new and still in its early stages.It requires certain skills and personnel with those skill sets to work with big data.As explained previously, the volume of large data sets is very high and they comprise a large variety which includes image, documents, audio and video ﬁles.It is important for people to have the skills to work with such data.In addition, big data needs to be processed at high speed.\n",
      "- 1) [6].Data challenges refer to charac- teristics of big data including volume, velocity, variety and veracity.Process challenges are related with the techniques needed for big data acquisition, inte- gration, transformation and analysis in order to gain insights from the big data.The data management challenges include challenges regarding data security, privacy, governance and cost/operational expenditures.Big data can be characterized by the seven Vs: volume, variety, veracity, velocity, variability, visualization and value.Volume refers to the large size of the datasets.It is fact that Internet of Things (IoT) through the development and increase of connected smartphones, sensors and other devices, in combination with the rapidly developing Information and Com- munication Technologies (ICTs) including Artiﬁcial Intelligence (AI) have con- tributed to the tremendous generation of data (counting records, transactions, tables, ﬁles etc.).The speed of data is surpassing Moore’s law and the volume of data generation introduced new measures for data storage i.e. exabytes, zettabytes and yottabytes.Variety represents the increasing diversity of data generation sources and data formats.Web 3.0 leads to growth of web and social media networks leading to the generation of different types of data.\n",
      "- The researcher selected ﬁve different industries for this research: healthcare, IT, retail, education and oil-gas.Companies in these sectors can use big data to solve many internal problems.Moreover, technology is con- stantly evolving and it is important to implement big data to gain a competitive edge in the marketplace.Big data is still new in the market and that is why it is very difﬁcult to ﬁnd appropriate technology which can work properly with companies’ operations.It is difﬁcult because big data cannot be stored in traditional data storages.As explained above, big data is very huge in terms of volume.So, it is not important to store such an amount of data in normal database storages.In addition to that, this data should be available in real time during the analysis process.So, it is important to ﬁnd the storage facilities which can provide such advantages.The second issue with big 260 B. Jadeja and T. Issa\n",
      "\n",
      "    Query: Describe the V's of big data\n",
      "\n",
      "    Answer:\n",
      "\n",
      "     The three Vs of big data are Volume, Variety, and Velocity. They characterize the unique aspects of big data. Volume refers to the large amount of data generated, Variety refers to the diverse types of data, and Velocity refers to the speed at which data is generated and processed. These Vs distinguish big data from traditional data, ensuring that organizations recognize the challenges and opportunities associated with managing and analyzing big data.\n",
      "</think>\n",
      "\n",
      "The three Vs of big data are Volume, Variety, and Velocity. These characteristics define the unique aspects of big data, distinguishing it from traditional data management practices. \n",
      "\n",
      "- **Volume** refers to the large amount of data generated, often measured in terabytes, petabytes, or even larger units like exabytes and zettabytes.\n",
      "- **Variety** indicates the diverse types of data, which can include text, images, documents, audio, video, and more, reflecting the broad range of data sources.\n",
      "- **Velocity** describes the rapid speed at which data is generated and processed, often exceeding traditional data handling capabilities.\n",
      "\n",
      "These Vs highlight the challenges and opportunities associated with big data management, emphasizing the need for efficient processing systems and advanced storage solutions.<｜end▁of▁sentence｜>\n"
     ]
    }
   ],
   "source": [
    "print(output_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "output_text = output_text.replace(prompt,\"\").replace(\"<｜begin▁of▁sentence｜>\",\"\").replace(\"</think>\",\"\").replace(\"<think>\",\"\").replace(\"<｜end▁of▁sentence｜>\",\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " The three Vs of big data are Volume, Variety, and Velocity. They characterize the unique aspects of big data. Volume refers to the large amount of data generated, Variety refers to the diverse types of data, and Velocity refers to the speed at which data is generated and processed. These Vs distinguish big data from traditional data, ensuring that organizations recognize the challenges and opportunities associated with managing and analyzing big data.\n",
      "\n",
      "\n",
      "The three Vs of big data are Volume, Variety, and Velocity. These characteristics define the unique aspects of big data, distinguishing it from traditional data management practices. \n",
      "\n",
      "- **Volume** refers to the large amount of data generated, often measured in terabytes, petabytes, or even larger units like exabytes and zettabytes.\n",
      "- **Variety** indicates the diverse types of data, which can include text, images, documents, audio, video, and more, reflecting the broad range of data sources.\n",
      "- **Velocity** describes the rapid speed at which data is generated and processed, often exceeding traditional data handling capabilities.\n",
      "\n",
      "These Vs highlight the challenges and opportunities associated with big data management, emphasizing the need for efficient processing systems and advanced storage solutions.\n"
     ]
    }
   ],
   "source": [
    "print(output_text)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
